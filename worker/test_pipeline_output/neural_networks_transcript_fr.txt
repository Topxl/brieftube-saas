Vidéo: 3Blue1Brown - Neural Networks Explained
URL: https://www.youtube.com/watch?v=aircAruvnKk
Langue: fr
======================================================================

Voici un 3. Il est griffonné et affiché avec une très basse résolution de 28×28 pixels. Mais votre cerveau reconnaît sans effort que c'est un 3. Prenez un instant pour admirer à quel point c'est incroyable qu'un cerveau puisse faire ça si facilement. En fait, ces images – celle-ci, elle, et celle-la aussi – sont aussi identifiées comme des 3, même si la valeur de chaque pixel varie énormément d'une image à l'autre. Les cellules photosensibles de vos yeux, qui s'activent en voyant ce 3, diffèrent de celles qui s'activent quand vous voyez l'autre 3. Mais quelque chose, dans ce fabuleux cortex visuel qu'est le vôtre, reconnaît ces trois comme exprimant la même idée, tout en distinguant les autres images comme porteuses d'idées distinctes. Imaginez que je vous demande de coder un programme qui reçoit une grille de 28×28 pixels comme celle-ci et renvoie un nombre unique entre 0 et 10, indiquant le chiffre détecté. La tâche passe de trivialement simple à terriblement difficile. À part si vous vivez dans une grotte, je pense que je n'ai pas besoin de vous convaincre de l'importance du machine learning et des réseaux de neurones aujourd'hui et dans le futur, Mais ce que je veux vous montrer ici, c'est ce qu'est réellement un réseau de neurones, sans besoin de connaissances préalables, et vous aider à visualiser son fonctionnement en tant qu'objet mathématique. J'espère que vous repartirez en comprenant la motivation de la structure de ces modèles et en saisissant ce que ça veut dire quand on lit ou qu'on entend qu'un réseau « apprend ». Cette vidéo se concentre uniquement sur l'aspect structurel des réseaux et la suivante abordera l'apprentissage. Ce que nous allons faire, c'est assembler un réseau de neurones qui pourra reconnaître des chiffres manuscrits. C'est l'exemple classique pour introduire le sujet, et je m'en tiens à cette approche très connue, car à la fin de ces deux vidéos je vous montrerai quelques bonnes ressources pour en apprendre davantage et expérimenter par vous-même, sur votre propre ordinateur. Il existe de nombreuses variantes de réseaux de neurones, et ces dernières années, la recherche sur ces variantes a explosé, mais dans ces vidéos introductives, nous nous concentrons sur leur forme la plus simple, la version ordinaire, dite "plain vanilla", sans fioritures. C'est vraiment un passage obligé pour comprendre ne serait-ce qu'une seule de ces variantes modernes plus efficaces, et croyez-moi, cette version est déjà bien assez complexe. Mais même dans cette plus simple forme, un reseau peut reconnaître des chiffres manuscrits, et ça, c'est quand même plutôt cool pour un ordinateur. Aussi, on verra par la même occasion qu'il ne répond pas non plus à 100% des attentes qu'on pourrait avoir. Comme leur nom l'indique, les réseaux de neurones s'inspirent du fonctionnement du cerveau, mais simplifions. Que sont les neurones et comment sont-ils reliés ? Pour l'instant, quand je dis « neurone », imaginez simplement quelque chose qui contient un nombre, plus précisément un nombre entre 0 et 1. Ca n'est rien de plus que ça. Par exemple, le réseau débute avec un tas de neurones correspondant aux 28×28 pixels de l'image d'entrée, soit 784 neurones au total. Chacun stocke un nombre représentant la valeur de gris d'un seul pixel, allant de 0 (pour le noir) à 1 (pour du blanc). Ce nombre dans le neurone est appelé son "activation", et l'image que vous devez avoir ici, c'est que chaque neurone est dit "allumé" quand son activation est élevée. Ainsi, ces 784 neurones forment la première couche de notre réseau. Regardons directement la dernière couche: celle-ci a 10 neurones, chacun représentant un chiffre. L'activation de ces neurones (encore un nombre entre 0 et 1) représente à quel point le système pense que l'image correspond à un chiffre donné. Il y a aussi quelques couches entre, qu'on appelle les "couches cachées", dont le rôle dans le processus de reconnaissance doit pour l'instant vous paraître très mystérieux. Dans ce réseau, j'ai choisi deux couches cachées avec 16 neurones chacune, et qu'on se le dise c'est un choix assez arbitraire. Honnêtement, vous allez voir, j'ai choisi deux couches parce que ça m'arrangeait pour expliquer l'architecture du réseau. Et 16 ? Eh ben... c'était juste un nombre qui rendait bien à l'écran. En pratique, il existe de nombreuses possibilités d'expérimentation avec la structure. Le fonctionnement du réseau est simple : l'activation d'une couche détermine celle de la suivante. Et bien sûr, le cœur du réseau en tant que mécanisme de traitement d'informations se résume à comment l'activation d'une couche entraîne l'activation de la couche suivante. C'est censé être analogue aux réseaux neuronaux biologiques, où l'activation de groupes de neurones en active d'autres. Le réseau que je vous montre ici, est déjà entraîné pour reconnaître des chiffres. Laissez-moi vous montrer ce que je veux dire. Ça veut dire que si vous lui donnez une image, ça active tous les 784 neurones de la première couche, en fonction de la luminosité de chaque pixel dans l'image. Ce schéma d'activations entraîne un motif spécifique dans la couche suivante, puis dans la suivante, qui au final donne un dernier motif dans la couche de sortie. Le neurone le plus activé dans cette dernière couche détermine le chiffre, pour ainsi dire "reconnu" par le réseau dans l'image. Et avant d'aborder les mathématiques derrière le passage d'une couche à la suivante, ou encore l'entraînement, parlons de pourquoi on peut s'attendre a ce que cette structure en couches agisse intelligemment. A quoi peut-on s'attendre, en fait ? Que peut-on espérer qu'il se passe dans ces couches intermédiaires ? Eh ben, lorsque vous ou moi reconnaissons des chiffres, nous assemblons différentes formes simples. Par exemple, un 9 possède une boucle en haut et une ligne à droite. Un 8 a aussi une boucle en haut, mais également une boucle en bas. Un 4 se décompose essentiellement en trois lignes distinctes, et ainsi de suite. Dans un monde idéal, on pourrait espérer que chaque neurone de l'avant dernière couche corresponde avec une de ces formes simples. Et qu'à chaque fois qu'on lui donne une image, disons une boucle en haut comme dans un 8 ou un 9, il y a certains neurones spécifiques qui s'activent presque à 1. Et je ne parle pas de cette exacte boucle de pixels, mais que n'importe quel vague motif de boucle vers le haut de l'image active ce neurone. Ainsi, passer de la troisième à la dernière couche ne requiert qu'un apprentissage de la combinaison de formes simples qui correspond à chaque chiffre. Bien sûr, cela ne fait que reporter le problème, puisque comment pourrions-nous reconnaître ces formes ou même apprendre quelles formes sont censées êtres les bonnes ? Et je n'ai même pas encore expliqué comment une couche influence la suivante, mais faites moi confiance, ça viendra ensuite. Reconnaître une boucle peut aussi se décomposer en sous-problèmes. Par exemple, on peut d'abord identifier les différents "petits traits" qui la composent De la même façon, une longue ligne comme dans les chiffres 1, 4 ou 7 peut être vu comme une seule longue bordure, ou bien alors comme l'assemblage de plusieurs traits plus petits. On peut donc peut-être espèrer que chaque neurone de la deuxième couche corresponde à ces petits traits. Peut-être que lorsqu'une image comme celle-ci arrive, elle active tous les neurones associés à 8 à 10 petits traits spécifiques qui, à leur tour, activent les neurones associés avec "une boucle en haut" et avec "une longue ligne verticale à droite", activant ainsi le neurone correspondant à un 9. Que cela reflète réellement le fonctionnement final de notre reseau, ça, c'est une autre histoire, que je traiterai une fois que nous aurons vu comment entraîner ce réseau. Disons que c'est ce qu'on peut espérer, une sorte de but avec cette structure en couches. De plus, imaginez l'utilité de pouvoir détecter des traits et des motifs pour d'autres tâches de reconnaissance d'images Et au-delà de la reconnaissance d'images, il y a des millions de tâches complexes qui peuvent être décomposées en couches d'abstraction successives. Par exemple, la reconnaissance vocale extrait d'un audio brut des sons, qui se combinent pour former des syllabes, qui elles-mêmes forment des mots, qui eux-mêmes forment des phrases, puis des idées abstraites, etc. Mais revenons au fonctionnement de tout ça: imaginez concevoir comment s'y prendre pour que l'activation d'une couche détermine celle de la suivante. L'objectif est d'avoir un mécanisme capable de combiner les pixels en traits, ou les traits en motifs, ou encore les motifs en chiffres. Pour un exemple précis, supposons que ce neurone spécifique de la deuxième couche doive détecter la présence d'un trait, disons, dans cette zone, ici. La question qu'on se pose c'est : quels paramètres le réseau doit-il avoir ? Quels réglages vont permettre de potentiellement capter ce motif ou n'importe quel autre motif de pixels, ou même de capter le fait qu'une combinaison de traits peut former une boucle, et tout le reste ? Et bien, pour cela, nous allons attribuer un poids à chaque connexion entre notre neurone et les neurones de la première couche. Ces poids, ce sont juste des nombres. Ensuite, on prend chacune des activations, et on calcule leur somme pondérée, selon ces poids. Je pense que c'est utile d'imaginer ces poids organisés dans une grille, avec des pixels verts pour les poids positifs et rouges pour les poids négatifs, et où la luminosité de chaque pixel donne une indication de leur valeur absolue. Maintenant, si on fixe à zéro presque tous les poids des pixels sauf dans cette région qui nous intéresse, où on donne des poids positifs, et bien la somme pondérée de toutes les valeurs des pixels correspondra simplement à l'addition des valeurs des pixels de cette région. Et si vous voulez vraiment détecter un trait ici, on peut utiliser des poids négatifs pour les pixels alentours. Ainsi, la somme sera maximale lorsque les pixels centraux sont lumineux et les alentours sombres. A priori, calculer ce genre de somme pondérée peut donner n'importe quel nombre,. Mais pour ce réseau, on ne veut que des valeurs d'activations entre 0 et 1. Donc ce qui est fait couramment, c'est de de passer cette somme pondérée dans une fonction qui vient comprimer la droite des réels entre 0 et 1. Et une fonction qui fait exactement ceci et qui est souvent utilisée, c'est la fonction sigmoïde, ou courbe logistique. En gros, des entrées très négatives tendent vers 0, tandis que des entrées positives tendent vers 1, et entre les deux, on a une augmentation régulière Ainsi, l'activation d'un neurone mesure en fait à quel point la somme pondérée est positive. Cependant, on ne veut peut-être pas toujours que le neurone s'active dès que la somme dépasse 0. Peut-être qu'on veut qu'il s'active uniquement si la somme dépasse, disons, 10. Autrement dit, on veut introduire un biais pour qu'il reste inactivé plus longtemps. Ainsi, on va simplement ajouter un autre nombre, ici -10, à cette somme pondérée avant de passer par la fonction sigmoïde qui la ramène entre 0 et 1. Ce nombre additionnel, c'est ce qu'on appelle: le biais. Donc les poids indiquent le motif détecté par ce neurone de la deuxième couche, et le biais nous indique à quel point la somme doit être élevée afin que le neurone ne soit vraiment activé. Et tout ça, c'est pour un seul neurone ! Chacun des autres neurones de cette couche est aussi connecté aux 784 neurones-pixels de la première couche, et chacune de ces 784 connexions a son propre poids associé. De plus, chaque connexion possède un biais, un autre nombre qu'on ajoute à la somme pondérée avant de la passer dans la fonction sigmoïde. Ca fait beaucoup, là, non ? Pour cette couche cachée de 16 neurones, cela représente 784 fois 16 poids plus 16 biais. Et ce n'est que la connexion entre la première et la deuxième couche. Les connexions entre les autres couches comportent également de nombreux poids et biais qui leur sont associés. Au total, ce réseau possède près de 13 000 poids et biais. 13 000 boutons à tourner, et qui modulent le comportement global du réseau. Alors, quand on parle d'apprentissage, ça correspond en fait à faire en sorte que l'ordinateur trouve seul le bon réglage pour chacun de ces nombres afin de résoudre le problème posé. Une expérience de pensée à la fois marrante et effrayante, ça serait d'imaginer essayer de paramétrer tous ces poids et tous ces biais à la main. Changer ces nombres pour que la deuxième couche détecte les traits, la troisième les motifs etc. Personnellement, je trouve ça plus satisfaisant que de simplement voir le réseau comme une boîte noire, car si le réseau ne fonctionne pas comme prévu, si on a une intuition sur le rôle de ces poids et biais, ça fait déjà un point de départ pour expérimenter et changer l'architecture du réseau pour l'améliorer. Ou bien, lorsque le réseau marche, mais pas pour les raisons auxquelles on s'attendrait, analyser ces paramètres est une très bonne manière de remettre en cause nos hypothèses, et voir en entier l'ensemble des solutions possibles. D'ailleurs, la fonction qu'on voit ici, c'est un peu lourd à écrire, vous ne trouvez pas ? Laissez-moi vous montrer une notation plus compacte pour représenter ces connexions. Le genre de notations que vous verrez forcément si vous choisissez d'en lire plus sur ce sujet.
212
00:13:41,380 --> 00:13:40,520
Organisons les activations d'une couche dans un vecteur colonne. Ensuite, disposons les poids dans une matrice, où chaque ligne correspond aux connexions entre une couche et un neurone spécifique de la couche suivante. Ca veut dire que prendre la somme pondérée des activations de la première couche selon ces poids, correspond à un des coefficients de la matrice produit de tout ce qu'on a à gauche ici. D'ailleurs, une grande partie du machine learning repose largement sur une bonne maîtrise de l'algèbre linéaire, donc si vous voulez comprendre les matrices et ce qu'une multiplication de matrice fait visuellement, consultez ma série sur l'algèbre linéaire, en particulier le chapitre 3. Revenons à notre expression : au lieu d'ajouter le biais à chaque valeur séparément, on regroupe tous ces biais dans un vecteur, que l'on ajoute ensuite au produit matriciel. Enfin, dernière étape, on va appliquer la fonction sigmoïde tout autour, et ce que c'est censé vouloir dire c'est qu'on applique la fonction sigmoïde à chaque composant du vecteur obtenu entre parenthèse. Ainsi, une fois la matrice de poids et les vecteurs exprimés symboliquement, on peut décrire entièrement la transition des activations d'une couche à la suivante de manière très concise, et très propre. Ce qui rend le code plus simple et rapide, car de nombreuses bibliothèques optimisent enormément le produit matriciel. Vous vous souvenez quand j'ai dit plus tôt que ces neurone, c'était simplement des objets qui contiennent des nombres ? Eh bien les nombres spécifiques qu'ils contiennent dépendent de l'image que vous lui donnez en entrée. Il est donc en fait plus juste de voir chaque neurone comme une fonction qui prend en entrée, les sorties de tous les neurones de la couche précédente et renvoie un nombre entre 0 et 1. En fait, tout le réseau n'est rien d'autre qu'une fonction. Une fonction qui prend 784 nombres en entrées et renvoie 10 nombres en sortie. C'est une fonction ridiculement complexe qui implique 13 000 paramètres sous forme de poids et de biais qui détectent des motifs, qui effectuent de nombreux produits matriciels et plein d'applications de la fonction sigmoïde, mais ça reste juste une fonction. D'une certaine manière, c'est même plutôt rassurant qu'elle soit aussi complexe. Si elle était plus simple, comment pourrions-nous espérer qu'elle puisse reconnaître des chiffres ? Et donc, comment cette fonction s'attaque à ce défi ? Comment ce réseau apprend-il à ajuster ses poids et biais simplement en observant des données ? Eh bien, c'est ce que je vous montrerai dans la prochaine vidéo et je creuserai aussi un peu plus sur ce que fait vraiment ce réseau en particulier. C'est à ce moment-là que je devrais dire "Abonnez-vous pour rester notifié quand je sors une nouvelle vidéo" Mais franchement, aucun de vous ne regarde vraiment ses notifications sur YouTube, si ? Enfait, je devrais plutôt dire "Abonnez-vous pour que le réseau de neurones qui gère l'algorithme de recommandations de YouTube soit amené à croire que vous voulez que le contenu de cette chaîne vous soit recommandé". Bref, restés à l'affût pour plus de contenu. Merci beaucoup à tous ceux qui ont soutenu ces vidéos sur Patreon. J'ai été assez lent pour avancer dans la série sur les probabilités cet été, mais je reprends  après ce projet. Donc les patrons, attendez-vous à avoir des nouvelles de ce côté-là. Pour finir, j'ai avec moi Lisha Li, qui a réalisé sa thèse sur les aspects théorique du deep learning et qui travaille actuellement dans une société appelée "Amplify partners" qui a gentiment aidé au financement de cette vidéo. Alors, Lisha, une chose sur laquelle j'aimerais revenir, c'est cette fonction sigmoïde. De ce que j'ai compris, les premiers réseaux de neurones l'utilisaient pour ramener la somme pondérée entre 0 et 1, motivée par l'analogie des neurones biologiques qui sont soit inactifs, soit activés. (Lisha) - Exactement (3B1B) - Mais peu de réseaux modernes utilisent encore la sigmoïde. (Lisha) - Oui (3B1B) - C'est un peu démodé, non ? (Lisha) - Oui ou plutôt, ReLU est bien plus facile à entraîner. (3B1B) - Et ReLU, ça veut dire « Unité Linéaire Rectifiée », c'est ça ? (Lisha) - Oui, c'est ce genre de fonction où vous prenez juste le maximum entre 0 et "a" où "a" est donné par ce que tu expliquais dans la vidéo, et c'est motivé, je pense, partiellement par une analogie biologique avec comment les neurones sont soit activés soit non, et donc, s'ils passent un certain seuil on a la fonction identité mais sinon, il ne sera tout simplement pas activé, c'est une sorte de simplification. L'utilisation de la fonction sigmoïde n'aide pas pour l'entraînement, ou en tous cas c'est vraiment difficile à entraîner à un certain niveau, et des gens ont simplement essayé ReLU et il se trouve que ça a marché vraiment bien pour ces réseaux incroyablement profonds. (3B1B) - Ok ! Merci Lisha.